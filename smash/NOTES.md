# ../Smash Development Notes

Generated from git commit history on 2025-08-01

## Development Timeline

### Commit 1: New feature: Add virtual controller. (2e64bd6)

```markdown
# NOTES.md

## Commit: 2e64bd6 - New feature: Add virtual controller.

This commit represents a significant leap forward in our project's ability to programmatically interact with the target game environment. The core problem we're solving here is how to feed input to a game (specifically, a Smash Bros. type game) from our emerging AI agent, particularly an LLM. Our `TODO.md` clearly outlines the need for a "Game loop" and an "agent" to "play until game over screen," and that requires a robust input mechanism. Directly injecting commands into arbitrary game processes is often brittle or platform-dependent. The most stable and widely compatible approach is to simulate a physical input device at the operating system level.

To achieve this, we've adopted a two-pronged technical approach. Firstly, we've formalized our Python project structure and dependency management by initializing **Poetry**. This brings `pyproject.toml` and `poetry.lock` into the repository, ensuring reproducible builds and a clean way to manage our external libraries. Secondly, and most critically, we've integrated the `python-uinput` library. This library provides a Pythonic interface to the Linux `uinput` kernel module, allowing us to create virtual input devices – in this case, a fully functional gamepad – that the operating system treats indistinguishably from a physical one. This is a powerful architectural decision: by mimicking a real device, we bypass potential game-specific input handling quirks and ensure maximum compatibility.

Dive into `virtual_controller.py` to see the implementation details. We've defined the standard set of gamepad `BUTTONS` (A, B, X, Y, shoulder buttons, etc.) and `AXES_SPECS` for the left analog stick's X and Y coordinates. The `run_controller_loop` function acts as a rudimentary "fuzzing" agent for now, emitting random button presses (70% probability) and axis movements (30% probability) at varied intervals. This provides an immediate way to test the virtual controller's functionality and its recognition by the system and any running game. Critically, we've implemented robust **graceful shutdown** mechanisms. Signal handlers for `SIGINT` (Ctrl+C) and `SIGTERM` ensure that when the script exits, the `reset_device_state` function is called. This function zeroes out all axis values (centering sticks) and releases all buttons, preventing the virtual device from being left in a "stuck" state if the script is interrupted. The `uinput.Device` is also wrapped in a `with` statement, guaranteeing its proper destruction.

A significant challenge when working with `uinput` is often permissions and kernel module loading. We've anticipated this by including an `OSError` catch in `main()` that provides helpful instructions to the user (`sudo modprobe uinput && sudo chmod 666 /dev/uinput`). This common troubleshooting step is now explicitly part of our error feedback, improving developer experience. The inclusion of a small `time.sleep(1.5)` after device creation also helps ensure the operating system has sufficient time to fully register the new virtual gamepad before input events begin. This initial `virtual_controller.py` lays the essential groundwork for our future LLM agent to take command and drive game interactions. We now have a reliable, low-level interface for our AI to "play" the game.
```

### Commit 2: Add virtual controller script and absl-py dependency (ae05f7e)

# NOTES.md

## Commit ae05f7e: Virtual Controller Revamp and Abseil Integration

This commit represents a significant step in hardening our virtual controller infrastructure, moving it from a basic, proof-of-concept script to a more robust and developer-friendly component. The primary drivers were a need for better operational visibility (logging), a standardized way to manage application lifecycle and command-line arguments, and a more realistic simulation of game controller input.

**Problem Solved & Technical Approach:**
The previous `virtual_controller.py` was rudimentary. Its logging relied on simple `print` statements, making it difficult to control verbosity or integrate with broader system logging. There was no formalized CLI argument parsing, and its input simulation was simplistic: buttons were pressed and released instantly. This "flicker" might not be ideal for training agents or testing game states that depend on held inputs. To address these, we made a core architectural decision: integrate `absl-py`. Abseil, being a common library for many Google projects, immediately provides battle-tested solutions for logging (`absl.logging`), application entry points (`absl.app`), and future CLI flag management.

**Architectural Decisions & Implementation Details:**
The adoption of `absl-py` necessitated a rewrite of the script's entry point (`main` now wrapped by `absl.app.run`) and a systematic replacement of `print()` calls with `logging.info()` and `logging.error()`. This enhances debuggability and allows for configurable log levels. Beyond this infrastructure, the core controller logic was refactored into modular functions: `build_device`, `reset_device`, and `run_event_loop`. This vastly improves readability and maintainability.

Crucially, the `run_event_loop` received a significant upgrade. We introduced a `held_buttons` dictionary, which now allows the virtual controller to simulate buttons being *held down* for a random duration before being released. This is a vital enhancement for mimicking real player behavior or allowing an agent to learn actions that require sustained input (e.g., charging a smash attack, holding a shield). The random sleep intervals between events were also substantially increased (from `0.03-0.20s` to `0.3-2.0s`). This change, while seemingly minor, slows down the input generation, making the virtual controller's actions less frantic and potentially more stable for game interaction and observation. Error reporting for `uinput` issues was also streamlined, providing clear instructions for common permission problems. Finally, proper signal handling via Abseil's app lifecycle ensures graceful shutdown and device reset on exit.

This commit lays a solid foundation for more complex virtual controller behaviors, better integration with future agent training frameworks, and overall improves the robustness and observability of our game simulation environment.

### Commit 3: Rewrite and expand virtual_controller.py (56dd0cf)

# NOTES.md

## Commit 56dd0cf: Virtual Controller Overhaul - From Mashing to Orchestrated Input

This commit represents a fundamental shift in how our virtual controller operates, moving it from a random button-mashing demo to a highly structured, deterministic, and programmable input system. The core problem we were solving was the complete lack of control and predictability over the agent's actions. The previous `virtual_controller.py` was merely a placeholder that sent arbitrary button presses and axis movements, making it impossible to implement any meaningful AI behaviors, let alone complex "combos" or precise reactions vital for a fighting game agent. We needed a robust mechanism for an AI to issue specific, timed sequences of controller inputs.

Our technical approach involved a comprehensive rewrite based on a producer-consumer model with strict frame-accurate scheduling. We abstracted controller inputs into core "primitives" like `press()`, `release()`, and `move_axis()`, along with a critical `wait()` primitive that allows for pauses measured in game frames. The real game-changer is the "Move Library," where each complex action (e.g., `jump()`, `dash()`, `strong_attack()`, `escape_roll()`) is implemented as a Python generator. These generators yield `(frame_offset, action_callable)` pairs, precisely defining when each primitive action should occur relative to the start of the move. This generator-based design is a key architectural decision, enabling composability (moves can `yield from` other moves, making combo definition elegant) and explicit temporal control. These "moves" are then encapsulated in a `Move` class, which also carries metadata like an `urgent` flag.

The system's execution backbone is the `control_loop`, which operates at a fixed 60Hz tick rate, meticulously processing the scheduled actions from the currently executing move. To decouple the AI's "planning" (which might run asynchronously) from the controller's "execution," we introduced a thread-safe `InstrumentedQueue`. This queue acts as the communication channel, where a `dummy_planner` thread (simulating our future AI agent) enqueues `Move` objects. A crucial feature is the urgent-move cancellation mechanism: if an urgent move is enqueued, it clears the queue and signals the `control_loop` to immediately abort the current move, allowing for reactive behaviors. After each move completes, the controller state is automatically reset (buttons released, axes centered), ensuring a clean slate for the next action. This entire setup provides the precise, deterministic "nervous system" our AI needs to genuinely interact with the game, paving the way for addressing the "Combos" and "Game loop" items in our `TODO.md`.

### Commit 4: Frame timing for moves standardized and lengthened: (bac7db7)

## `bac7db7`: Fine-Tuning Frame Timings for Realistic Agent Input

This commit represents a critical step in making our virtual controller's inputs more robust and realistic, addressing a core challenge for building sophisticated AI agents: the precise timing of character actions. Previously, the input sequences for moves in `virtual_controller.py` were often too rapid or lacked the nuanced delays common in fighting games, leading to potentially unreliable character behavior or actions that simply didn't "feel right" when observed. The problem this commit solves is to align our simulated controller inputs with established "fighting game input best practices," thereby laying a more stable foundation for agent development, especially for complex maneuvers and combos as outlined in our `TODO.md`.

Our technical approach centered on standardizing and significantly lengthening the frame delays between stick motions and subsequent button presses, as well as extending the duration of button holds. For most standard attacks (`strong_attack`, `high_attack`, etc.), the button press delay after a stick input was increased from an immediate 1-2 frames to a more deliberate 4-6 frames. Button hold durations were similarly extended, now typically ranging from 8 to 18 frames. Smash attacks, which inherently imply a "charge" or precise timing window, received even more substantial adjustments: their initial stick return delay was extended to 3 frames (from 1), the button press delay to 6 frames (from 2), and the button hold window was dramatically lengthened to 17 frames (from 10). This level of granular control, leveraging our existing generator-based move definitions (`yield (frame, action)`), proved highly effective for fine-tuning. A notable refinement was also made to `escape_roll`, where `yield from guard()` was replaced with a direct `(0, press('Z'))` – this explicit, immediate guard press at frame zero improves correctness and removes potential implicit delays or state dependencies that might arise from delegating to another generator.

One of the key challenges overcome was ensuring proper state synchronization. For combined moves like `dashing_attack` or `jumping_attack`, it's crucial that the character first enters the "dash" or "jump" state *before* the attack button is registered. This commit addresses this by introducing additional, specific frame delays (e.g., 6 frames for `dashing_attack` and 4 for `jumping_attack`) after the initial movement input but prior to the attack button press. Interestingly, `dashing_attack` now explicitly passes `dash_frames=5` to its internal `dash()` call, overriding the new global `dash` default of 6. This specific override highlights the detailed level of tuning required to get these combined actions just right, ensuring the dash has sufficient duration for the attack to correctly register during the dash animation. To validate these extensive timing adjustments, the `dummy_planner` was updated to enqueue `dashing_attack`, serving as an immediate test case for the newly tuned sequences. This comprehensive update to our input timings not only makes our agents' actions more believable but also provides a more reliable bedrock for developing sophisticated combo logic and reactive gameplay.

### Commit 5: Initial docker, etc. (e1ed2a7)

## NOTES.md

### Commit `e1ed2a7`: Initial Docker, Headless Graphics, and LLM Integration

**Development Story:** This commit marks a monumental leap forward, shifting our `smash` project from a mere local prototype to a robust, reproducible, and *intelligently controlled* virtual gaming agent. The core problem being solved was enabling a Large Language Model (LLM) to "see" the game, "think" about its state, and "act" by controlling a virtual gamepad, all within an isolated and consistent environment. This is the foundation for turning our `TODO.md` goal of an LLM-driven game loop into a reality.

**Technical Approach & Architectural Decisions:**
The primary technical approach was a "Docker-first" strategy, encapsulating all dependencies and the application logic within a container. This addressed immediate reproducibility challenges and laid the groundwork for future scalable deployments. Key to this was setting up a **headless graphical environment** using `Xvfb`, `xauth`, and `xorg` inside the Docker container. This is crucial because RetroArch, the game emulator, requires a display, and our screenshot mechanism (`mss`) needs to grab pixels from *somewhere*. The `Dockerfile` explicitly installs these graphical dependencies, along with `tesseract-ocr` for future image-to-text analysis (the LLM's "eyes"), and `libudev-dev` as a "KEY FIX" for `python-uinput` to function correctly for virtual gamepad creation.

A significant architectural decision was to install **RetroArch and its N64 core (ParallelN64)** directly within the Docker container using **Flatpak**. This dramatically simplifies dependency management for RetroArch, which can be notoriously complex to compile or install system-wide. We then leveraged RetroArch's **UDP network command interface** (`retroarch.py`) to programmatically control it – loading ROMs, loading savestates, querying status, and crucially, triggering screenshots. This creates the primary communication channel between our Python application and the emulator.

**LLM Integration and Control Flow:**
The most exciting development is the deep integration of a multi-modal LLM (Google Gemini 1.5 Flash/Pro via `langchain-google-genai`). The `virtual_controller.py` has been transformed:
1.  **Observation (`llm_planner`):** It now uses `mss` to capture a live screenshot of the `Xvfb` display, which is then base64 encoded. This image data, combined with a carefully crafted prompt (and potentially RetroArch status info via `get_status`), is fed directly to the Gemini LLM using LangChain's `ImagePromptTemplate`. This effectively gives our LLM "vision."
2.  **Action (`llm_planner` + `@tool` decorators):** All our game "moves" (e.g., `jump()`, `normal_attack()`, `dash()`) are now decorated with `@tool`. This exposes them as callable functions that the LLM can *choose* to invoke. The LLM's response is no longer just text; it's a structured tool call. This chosen tool is then dynamically executed by our `control_loop`, which queues up the gamepad inputs via `python-uinput`. This forms the LLM's "hands."
3.  **Game State Management:** The `make_retroarch` function now handles the complete initialization sequence: launching RetroArch with a specific ROM and core, and then loading a savestate. This ensures the game starts in a consistent and ready-to-play state for the LLM. A minor but important implementation detail for Super Smash Bros. was the remapping of `BTN_B` to `X` and `BTN_X` to `B` to correctly align with Smash's common attack mapping (A for normal, B for special).

**Challenges Overcome & Next Steps:**
Getting the headless graphical stack and `uinput` permissions correctly configured within Docker was a primary challenge (hence `libudev-dev` and the `--device=/dev/uinput` flag in `rebuild_and_run.sh`). Ensuring RetroArch's network commands were active and reliably accessible from within the container was also key. The `entrypoint.sh` script handles the `Xvfb` lifecycle, guaranteeing the display is ready before RetroArch is launched. Future work will involve more sophisticated prompt engineering to guide the LLM's decision-making for optimal gameplay, as well as developing robust methods for parsing game state beyond simple OCR (e.g., character positions, damage percentages) to provide richer context to the LLM. This commit establishes a powerful framework for an AI agent to learn and play complex games.

### Commit 6: Monolithic latest ubuntu (43e9bde)

## NOTES.md Entry: 43e9bde - Monolithic latest ubuntu

This commit represents a significant overhaul of our Docker build process, addressing persistent issues with build stability and the complexities of managing native Python extensions within a multi-stage Docker environment.

**The Problem:** The previous multi-stage Dockerfile, while aiming for smaller final images, introduced considerable complexity and fragility. Specifically, the `python:3.9-slim AS builder` stage, responsible for installing Python dependencies and C extension build tools (`build-essential`, `python3-dev`, `libudev-dev`), often led to subtle runtime failures. Copying a virtual environment from a lean Python builder to an `ubuntu` runtime image created a potential for ABI mismatches or missing runtime shared libraries that weren't explicitly copied, even if the build *succeeded*. The explicit "THIS IS THE KEY FIX SECTION" comment in the old Dockerfile for these packages clearly indicated a source of pain and repeated debugging efforts.

**The Technical Approach & Architectural Shift:** The core architectural decision here was to transition to a **monolithic Dockerfile** based directly on `ubuntu:latest`. This simplifies the entire build chain. Instead of separating build and runtime concerns into distinct stages, all necessary system dependencies – including the Python build tools (`python3-dev`, `build-essential`, `libudev-dev`) and Python itself – are now installed directly within the main `ubuntu:latest` image. Poetry itself is installed system-wide via its official installer script, and then configured for in-project virtual environments to be created by the non-root application user. This ensures that the Python dependencies, especially those with native components like `python-uinput` or `mss`, are built and linked against the *exact* system libraries that will be present at runtime, eliminating the environment discrepancies that previously plagued our builds.

**Benefits & Key Details:** This revised approach dramatically simplifies our Dockerfile, making it easier to reason about, debug, and maintain. The primary gain is **robustness and predictability**: if it builds, it will run. While `ubuntu:latest` provides a very fresh base image (which is generally good for getting the latest RetroArch or Flatpak updates), it inherently means a slightly larger image size than a highly optimized multi-stage build *could* achieve. However, for an application tightly coupled to system-level libraries for controller emulation and screen capture, the stability and reduced debugging overhead are well worth this trade-off. Crucially, the Flatpak installation of RetroArch and its cores, along with user creation and ROMs setup, remains consistent, ensuring seamless integration with the newly robust Python environment. This change lays a much more solid foundation for further development, removing a significant source of "works on my machine, not in Docker" type issues.

### Commit 7: Add a few files. (b5b2430)

---
**Commit:** `b5b2430` - Add a few files.

### Development Story: Initializing the `smash` Subproject & Laying the Foundation

This commit marks the foundational setup for the new `smash` subproject, addressing the critical initial challenges of establishing a workable development environment and outlining the immediate, high-level engineering goals. Before any core game logic could be implemented, we needed a dedicated workspace, a streamlined mechanism for rapid iteration, and a clear roadmap for the `virtual_controller.py` and its interaction with the game. This commit provides precisely that, setting the stage for subsequent feature development.

The technical approach taken focuses heavily on optimizing developer workflow and structuring the project for future growth. The introduction of `smash/run.sh` is a deliberate step towards a highly responsive development loop. By leveraging `git ls-files . | entr -r poetry run python virtual_controller.py`, we've engineered an auto-reloading setup: any saved changes to tracked files within the `smash` directory will automatically restart the main `virtual_controller.py` script via `poetry`. This drastically shortens the feedback cycle, which will be invaluable when iterating on game loop mechanics and agent behaviors. Complementing this, `smash/run_docker.sh` provides a simpler, non-watching execution path (`poetry run python virtual_controller.py`) for scenarios where continuous restarts aren't needed, like potential production environments or containerized deployments. This clear separation of development and "production-like" execution acknowledges their distinct operational needs.

Architecturally, this commit reveals key decisions that inform the project's overall structure. The `smash/utils` symlink, pointing to `../utils`, signifies an intention to operate within a monorepo-like structure where common helper functions or shared modules at the project root can be easily consumed by subprojects without complex Python path manipulations or code duplication. This promotes reusability and maintains a single source of truth for shared components. Crucially, the accompanying `smash/TODO.md` file serves as the initial blueprint for the `smash` project's core functionality. It outlines two primary areas: "Combos" and a detailed "Game loop." The "Game loop" description ("Save some state with all four; play until game over screen (agent); figure out who won; keep score; start again; let it go for hours.") is particularly significant, as it explicitly defines the initial scope and the long-running, autonomous nature of the simulation we aim to build, emphasizing persistent state, agent-driven termination, and continuous play.

### Commit 8: Refactor: Improve Docker build and runtime for smashbot (c497aa6)

## NOTES.md: Docker Refactor for `smashbot` (c497aa6)

This commit marks a significant refactoring of our `smashbot` Docker setup, primarily focused on improving the build's reliability, runtime stability, and overall developer experience. The previous Docker configuration was proving somewhat brittle, especially around RetroArch core management and file system permissions, leading to frustrating rebuilds and runtime errors.

**Problem Being Solved & Technical Approach:**

The core problem was an inconsistent and often fragile Docker build and runtime environment. Specifically:

1.  **RetroArch N64 Core Management:** Previously, we relied on `flatpak install` for the `ParallelN64` core system-wide. This proved problematic, potentially due to Flatpak's sandboxing within a Docker context or permission issues for the non-root user. The solution was a **crucial shift**: instead of Flatpak, the `Dockerfile` now directly `curl`s the `parallel_n64_libretro.so.zip` from the Libretro buildbot and `unzip`s it into the `smashuser`'s RetroArch cores directory (`/home/$USERNAME/.var/app/org.libretro.RetroArch/config/retroarch/cores`). This gives the non-root user direct ownership and control over the core, simplifying paths and permissions.

2.  **Docker Build Context and Symlinks:** A subtle but common Docker gotcha was causing issues: symlinked directories (like our `utils/` helper module) were not always correctly copied into the build context using the standard `docker build .` command. This commit introduces a robust workaround in `rebuild_and_run.sh`: `tar -czh ... | docker build -t "$IMAGE_NAME" -`. The `-h` flag (or `--dereference`) in `tar` ensures that symlinks are dereferenced *before* being added to the tarball, effectively copying the *contents* of the linked directory rather than the symlink itself. This guarantees `utils/` and any other symlinked application code are properly present in the container. The temporary debugging `RUN ls -la` lines in the `Dockerfile` clearly highlight the struggle that led to this elegant solution.

3.  **User Permissions and Security:** The `Dockerfile` now creates the `smashuser` without hardcoding specific `UID`/`GID` values, allowing the system to assign them. More importantly, the `USER $USERNAME` switch happens much earlier in the build process. This adheres to the principle of least privilege, ensuring that subsequent commands (like `mkdir -p` for cores, or `COPY . .`) create files and directories directly owned by `smashuser`, avoiding potential permission conflicts that could arise from actions performed as root.

4.  **Xvfb Startup Robustness:** The `entrypoint.sh` now includes significantly improved logging for Xvfb startup failures. It attempts to `cat` common Xvfb log file paths and provides `pgrep`/`ps aux` output if Xvfb doesn't come up, greatly assisting in diagnosing display-related issues within the headless environment.

**Key Architectural Decisions & Implementation Details:**

*   **Decoupling Core Installation:** Moving from a Flatpak-managed core to a direct download into the user's home directory simplifies the dependency chain within the container and provides clearer ownership.
*   **Explicit Context Handling:** The `tar -h | docker build -` pattern in `rebuild_and_run.sh` is a critical architectural decision for projects with symlinked internal modules, ensuring reproducible and correct builds.
*   **Environment Variable Management:** The `GOOGLE_API_KEY` is now explicitly declared as an `ENV` in the `Dockerfile` (as a placeholder for documentation) and passed at runtime via `-e GOOGLE_API_KEY` in `rebuild_and_run.sh`, clearly defining how external secrets/configurations are handled.
*   **Dependency Slimming:** `entr` and `git` were removed from `apt-get` installs. This is a direct consequence of `entrypoint.sh` now calling `run_docker.sh` (which presumably simplifies the application's startup logic, no longer needing `git ls-files` or `entr` for watch-mode development within the container). `unzip` was added to support the new core download approach.
*   **Hardcoded Entrypoint Path:** A minor detail, but the `ENTRYPOINT` path was hardcoded to `"/home/smashuser/app/entrypoint.sh"` because `ARG` variables are not expanded in the exec form of `ENTRYPOINT`.

This refactor lays a much stronger foundation for `smashbot`'s Docker deployment, making it more resilient to build environment changes and significantly easier to debug. It directly supports the "Game loop" TODO by providing a stable environment for long-running, automated tests, allowing `smashbot` to "go for hours" without unexpected container failures due to core loading or file access issues.

### Commit 9: Integrates LiveKit server into the Docker setup. (d5fa999)

# NOTES.md

### Commit `d5fa999`: Integrating LiveKit for Real-Time Interaction

This commit marks a significant leap towards enabling real-time video/audio streaming and remote control for our Smash bot, directly integrating the LiveKit server into our Docker setup. This foundational work addresses the critical need for an interactive visual and audio feedback loop, directly supporting the "Game loop" and "Combos" sections of our `TODO.md` that envision agents interacting with the game over extended periods.

**Problem Solved & Technical Approach:**

The core problem was establishing a robust, low-latency communication channel for the bot's game output (video, audio) and potentially for external control signals. Running a full game with agents demands continuous visual feedback and the ability to process audio cues. LiveKit, a powerful WebRTC SFU, is the ideal candidate for this.

Our technical approach for integration involved a three-pronged attack across our build and runtime scripts:

1.  **Dynamic LiveKit Server Provisioning (`rebuild_and_run.sh`):** Instead of bundling a static LiveKit binary, the `rebuild_and_run.sh` script now intelligently fetches the *latest* LiveKit server release tarball directly from GitHub. This tarball is then passed as a build argument (`LIVEKIT_TARBALL_ARG`) to the `Dockerfile`. This is a neat trick: it keeps our source repository clean, ensures we're always pulling the most recent stable version, and decouples the LiveKit server's release cycle from our own. Crucially, the script cleans up the downloaded tarball from the host machine *after* the Docker build, maintaining a clean build environment. Network ports `7881` (websocket), `7880` (HTTP), and `1935` (RTMP) are exposed, making the LiveKit server accessible from outside the container.

2.  **Containerized LiveKit Server Installation (`Dockerfile`):** The `Dockerfile` now accepts the `LIVEKIT_TARBALL_ARG`, copies it into the build context, and then efficiently extracts *only* the `livekit-server` executable to `/usr/local/bin/`. This minimizes image bloat. Essential dependencies for video processing and streaming, such as `ffmpeg`, `libx264-dev`, and `tar`, were added. Furthermore, the `smashuser` is now added to the `audio` group, anticipating the need for direct audio device access for LiveKit or other audio processing within the container.

3.  **Robust Multi-Process Orchestration (`entrypoint.sh`):** This is where the container's runtime lifecycle got a significant upgrade. With Xvfb, the main application, potential `ffmpeg` processes for streaming, and now the LiveKit server all running concurrently, robust process management is paramount. The `entrypoint.sh` was refactored to implement a comprehensive signal trapping mechanism (`trap cleanup SIGINT SIGTERM SIGQUIT`). This `cleanup` function ensures that upon receiving a shutdown signal (e.g., from `docker stop`), all spawned processes—Xvfb, the main application, FFmpeg, and the LiveKit server—are gracefully terminated. This prevents orphaned processes and ensures a clean container shutdown. Additionally, the Xvfb startup logic was significantly hardened with detailed logging, checks for premature Xvfb death, and enhanced diagnostic output, making debugging startup issues much easier.

**Key Architectural Decisions & Challenges Overcome:**

The decision to run LiveKit *inside* the main bot container, rather than as a separate service (e.g., in a separate Docker Compose service), simplifies deployment for our current stage. It makes the bot a self-contained unit capable of providing its own real-time streaming capabilities. This implies the bot itself will act as a LiveKit participant, publishing its game output.

A significant challenge overcome was the orchestration of multiple independent, long-running processes within a single container. The initial `entrypoint.sh` was simple, but adding LiveKit and `ffmpeg` necessitated a more sophisticated approach to ensure all processes start correctly, communicate (implicitly, via display/audio servers), and, most importantly, shut down cleanly. The `trap` and `cleanup` pattern in `entrypoint.sh` is a critical architectural improvement for container resilience. We also faced the practical challenge of integrating a third-party binary without bloating our image or requiring manual pre-downloads, which was elegantly solved by the dynamic download-and-build-arg approach.

This commit lays the groundwork for high-fidelity, real-time interaction with the Smash bot, moving us closer to persistent game sessions and sophisticated agent-based gameplay as outlined in our `TODO.md`. The ability to stream the game's output directly opens doors for advanced visual AI feedback loops and remote human supervision or interaction.

### Commit 10: feat: Integrate LiveKit streaming and Roboflow vision for state detection (c0a1faa)

Here's a detailed `NOTES.md` entry for this commit:

## Commit: c0a1faa - feat: Integrate LiveKit streaming and Roboflow vision for state detection

This commit represents a monumental leap in our agent's capabilities, fundamentally shifting it from a "blind" controller to one that can actively "see" and interpret the game world. The core problem we aimed to solve was the agent's inability to perceive the dynamic, real-time visual state of Super Smash Bros. Without this perception, its decision-making was limited to pre-programmed sequences or simplistic state assumptions, hindering its strategic depth and adaptability. We needed to imbue our agent with eyes.

Our technical approach centers around building a robust, real-time video streaming and computer vision pipeline. Architecturally, we've introduced LiveKit, a WebRTC Selective Forwarding Unit (SFU), directly into our Docker container. The `rebuild_and_run.sh` script now automatically fetches the latest LiveKit server release during the build process, ensuring we're always running an up-to-date SFU. The `entrypoint.sh` orchestrates a sophisticated video capture and streaming flow: Xvfb, our virtual display server, now feeds its screen output directly to FFmpeg. FFmpeg, in turn, encodes this video into a WebM stream using `libvpx-vp9` and pushes it via the WHIP protocol to the LiveKit server running locally within the container. This establishes a low-latency, real-time video feed of the game environment.

Complementing this streaming backbone is the integration of Roboflow's computer vision capabilities. We've added the `inference-sdk` to our Python dependencies, enabling `virtual_controller.py` to capture screenshots from the virtual display, send them to a dedicated Roboflow workflow for object detection (specifically, identifying "Mario" and "DonkeyKong" and their bounding boxes), and receive a structured summary. A critical architectural decision here was to offload the heavy computational burden of vision processing to Roboflow's specialized serverless infrastructure, rather than attempting to run complex ML models directly within our constrained Docker environment. This design keeps our local agent lean and focused on control and planning, while leveraging robust external services for perception.

The most significant impact of these integrations is on the LLM planner. It now receives highly context-aware prompts that include precise character position data (X/Y coordinates, bounding box information) extracted from the vision model's output. This shift from abstract game state (e.g., "player 1 turn") to concrete visual state (e.g., "Mario at x=300, y=750, Donkey Kong at x=450, y=760") allows the LLM to reason about spatial relationships, proximity, and tactical positioning. To fully leverage this enhanced perception, we've increased the `move_queue` size from 2 to 8, allowing the LLM to plan more extensive action sequences based on its new understanding. Furthermore, a detailed refactoring and tuning of character move frame timings (e.g., jump frames, attack durations) was undertaken to ensure that the LLM's precise, visually-informed actions translate into effective and responsive in-game maneuvers. Lastly, enabling LangSmith tracing provides crucial debugging and observability into the LLM's decision-making chains, which is invaluable for understanding and refining its behavior as it learns to navigate this visually rich environment.

### Commit 11: First version of slides for ODSC talk (3aa25cf)

## NOTES.md Entry for `3aa25cf` - ODSC Presentation Architecture Review

This commit, `3aa25cf` ("First version of slides for ODSC talk"), is significant not for introducing new functional code, but for providing the first comprehensive, public-facing documentation of our Super Smash Bros AI agent's development journey and core architecture. It essentially distills months of prototyping and iteration into a clear narrative, highlighting the problems we faced and our technical solutions.

The central problem tackled by the project, as articulated in these slides, is: "How the hell do you teach an LLM to fight?" Specifically, how to enable a multi-modal LLM like Gemini to play a complex, real-time game requiring constant perception, strategic reasoning, and precise, rapid action. Our development story, laid out in the presentation, progressed through several phases, each addressing a critical challenge.

Our core technical approach revolves around a **Multi-Modal Reasoning Loop**, which is the architectural cornerstone. Key architectural decisions and the challenges they addressed include:
1.  **Bridging Perception Gaps:** Early attempts with LLMs alone ("Gemini Sees, but Doesn’t Understand") highlighted their struggle with implicit spatial reasoning from raw pixel data. The solution was to introduce a dedicated **Vision Model (e.g., Roboflow)** to explicitly detect and localize game entities like Mario, providing a structured input to the LLM. "Sometimes, seeing really *is* believing."
2.  **Decomposing Strategy from Tactics:** LLMs are powerful for high-level reasoning but fundamentally lack the precision and real-time responsiveness for frame-perfect game inputs. We solved this by separating the LLM's role into high-level "intent" generation (e.g., `["short hop fair", "dash grab"]`) and delegating the low-level, frame-perfect execution to a pre-programmed **"Frame-Precise Translator" with a Queue**. This system consumes the LLM's strategic output and translates it into a sequence of precise button presses, essentially giving the LLM "tools" to call, like `use(move="ledge_guard", target="Mario")`. This was a pivotal architectural decision, abstracting away the micromanagement from the LLM.
3.  **Managing Latency:** The ~2-second round trip for the full vision-LLM-action loop presented a major challenge to reactive gameplay. This forced us to design the agent for **anticipatory planning** rather than immediate reaction, where the agent predicts and acts for the near future rather than the absolute present.
4.  **Debugging the "Agentic Mind":** Understanding *why* the LLM made certain decisions (e.g., "Why did DK down-smash into thin air?") was critical for iteration. The use of **LangSmith** for tracing the LLM's thought process became an essential debugging tool, providing visibility into the agent's internal state and reasoning.

The slides also outline our future roadmap, emphasizing continued efforts to reduce latency (e.g., local vision models, faster LLMs like Gemma) and the exciting potential of a hybrid approach combining LLMs for strategy with Reinforcement Learning for fine-tuned tactics. This commitment, while not directly impacting the codebase's functionality, provides invaluable context and serves as a blueprint for the "development story" of the entire project, underlining the guiding principle that "strategy and tactics belong to different minds."

### Commit 12: feat(slides): Add presentation for the Smash Bros AI project (4c455ce)

---
title: "NOTES.md: Commit 4c455ce - Smash Bros AI Presentation"
author: Peter Danenberg
date: Mon Jun 30 12:51:42 2025 -0700
---

### Commit 4c455ce: feat(slides): Add presentation for the Smash Bros AI project

This commit marks a significant step not in the Smash Bros AI's capabilities, but in its *documentation and communication*. The primary problem being solved here was how to effectively articulate the complex development journey of our Gemini-powered Smash Bros agent to a technical audience, presumably for a conference (ODSC mentioned in the slides) or internal review. We've introduced a full-fledged [Slidev](https://sli.dev/) presentation (`smash/slides/`) that encapsulates the core development story, challenges, and architectural insights of the AI project.

The technical approach taken for the *presentation itself* was to leverage Slidev's Markdown-based framework, which allowed for rapid prototyping and easy inclusion of code snippets, diagrams (like the `design.dot` Graphviz diagram converted to `design.png`), and embedded media (e.g., `dk-flail.mp4` demonstrating early chaos). Key implementation details for the presentation include the setup for Vercel/Netlify deployment, ensuring it's easily publishable, and the inclusion of boilerplate Slidev components and styling.

More importantly, the content of this slide deck serves as a high-level retrospective on the AI's development. It outlines the core architectural decisions of the AI: a modular `Emulator -> AI Planner -> Controller -> Emulator` loop, with the AI Planner explicitly leveraging a **Multimodal LLM (Gemini)** for high-level reasoning and a **Vision Model (Roboflow)** for perception. It details the challenges overcome, such as the LLM's initial struggle with implicit spatial reasoning (solved by the vision model) and the critical need to translate Gemini's high-level strategic *intent* into precise, frame-perfect game actions via pre-programmed "tool" executions. The inherent 2-second latency of the LLM loop forced a shift towards anticipatory planning. Finally, the presentation highlights the use of debugging tools like LangSmith to unravel the AI's "beliefs" and understand its decision-making process, showcasing our approach to debugging a system where the "mind" is a black box. This commit, while seemingly just a presentation, is a rich summary of the AI project's foundational development story.

### Commit 13: refactor(slides): Improve layout and add new content (c1b7415)

## NOTES.md Entry: c1b7415 - Refine Agent Architecture Presentation with "Step 0"

This commit significantly refines our presentation of the game-playing agent's architecture, specifically by introducing a new, more flexible slide layout and, crucially, detailing a foundational concept: "Frame-Perfect Actions." The primary problem addressed here was twofold: enhancing the clarity and structure of our technical explanations in the slides, and formally introducing the very first, low-level abstraction layer that underpins our agent's control scheme.

To achieve improved presentation, we've implemented a new `two-cols-header` slide layout. This involved defining a custom CSS grid (`.slidev-layout.two-cols-header` utilizing `grid-template-rows`, `row-gap`, and `column-gap`) to allow for dynamic, side-by-side content – perfect for pairing a concept explanation with a code example. This architectural decision for the slide system itself enables a more engaging and digestible explanation of complex technical concepts.

The most significant architectural reveal in this commit is the introduction of "Step 0: Frame-Perfect Actions." This solves the critical problem of abstracting away the tedious, precise timing of low-level controller inputs. Instead of our high-level agent having to reason about "move stick up, wait 4 frames, press A," it can now simply think in terms of a "high attack." The technical approach is elegant: each high-level move is defined as a Python generator (`@tool` decorated function) that `yield`s a sequence of `(frame, action)` tuples. This allows for incredibly precise, pre-defined action sequences to be executed by a simple call, greatly simplifying the agent's reasoning task while maintaining pixel-perfect control. This also firmly establishes that these high-level moves will be exposed as "tools" to our Multimodal LLM, integrating directly into the "Tool use" component of our design.

Implementation details worth noting include the `v-clicks` directives for progressive content revelation, making the new "Step 0" explanation flow smoothly. We've also refined the estimated "reasoning loop" duration from "~2 sec" to "~1-2 sec," reflecting ongoing analysis or target optimizations for the agent's real-time performance. This commit effectively solidifies the critical, low-level interface between our AI and the game, setting a robust foundation for building more complex agent behaviors on top of these precise, pre-defined action primitives.

### Commit 14: refactor(slides): Improve layout and add new content (d60cddb)

### NOTES.md Entry: d60cddb - Reframing the Agent's Core Reasoning and Action Loop

This commit marks a significant step in formalizing our agent's interaction model with the game, moving us closer to tackling the overarching "Game loop" and "Combos" defined in `TODO.md`. The primary problem solved here was to establish a clear, structured way for the multimodal LLM to reason and act, replacing vague notions with concrete architectural decisions and demonstrable examples.

Our technical approach centers on defining a "Reasoning Loop" for the agent. This crucial architectural decision, now visually documented with `reasoning-loop.dot` and its `.svg` rendering, outlines a continuous cycle: the agent *perceives* the game state (via the `Game Screen`), the `Multimodal LLM` *reasons & selects* an action using `Tool Use`, which then *translates & executes* into low-level commands via the `Virtual Controller`, finally *acting on* the `Game`. This abstraction is key to enabling the LLM to operate at a higher semantic level.

A core implementation detail addressing the "action" part of this loop is the introduction of "high-level moves." These are not just abstract concepts; they're concretely implemented as Python generators that `yield` sequences of precise, frame-perfect controller actions. This tackles the inherent challenge of low-level, frame-perfect execution in a fighting game, abstracting it into digestible "verbs" for the LLM (e.g., "high_attack" instead of raw controller inputs). To immediately validate and demonstrate this, `virtual_controller.py`'s `dummy_planner` has been temporarily focused solely on executing the `high_attack` move, with a significantly increased `time.sleep` interval. This allows for clear, repeatable testing and visual verification (as seen in `dk-high-attack.mp4`) of these complex, timed actions, ensuring our foundational "vocabulary" of moves behaves as expected before integrating a full LLM planner.

Finally, this commit also solidifies project standards. The updated `slides.md` now features the reasoning loop diagram and integrates the visual demonstration of a high-level move, significantly improving the clarity of our development narrative. Furthermore, the introduction of `CONVENTIONS.md` is a testament to our commitment to maintainability and collaborative development, especially crucial when dealing with generated assets like SVGs and a growing codebase.

### Commit 15: Random slide (3066d25)

# NOTES.md

## Commit: 3066d25 - Random slide

This commit marks a significant leap towards developing a proper agent for Super Smash Bros., moving beyond purely scripted demonstrations towards an autonomous, albeit currently simplified, decision-making loop. The overarching problem addressed here is the initial integration of a large language model (LLM) to act as our "planner" or "agent brain," a direct step towards fulfilling the `TODO.md` goal of an agent-driven game loop. The commit's title, "Random slide," likely refers to the demonstration of this early, randomized (by the LLM picking *any* attack) agent in a presentation context, reflected by the updates to `slides.md`.

Our technical approach centers on leveraging the multimodal capabilities of a Gemini model (`gemini-1.5-flash-latest`) to select sequences of pre-defined game actions. Each atomic game action (e.g., `normal_attack`, `jump`) is encapsulated as a Python generator function, adorned with the `@tool` decorator. These tools yield tuples of `(frame_offset, action)`, allowing for granular, multi-frame maneuvers like dashes, charged attacks, or combos to be defined precisely. The LLM's role is to receive game state (currently a screenshot and a hardcoded textual summary, more on that below) and respond with a list of "tool calls." These tool calls are then dispatched as `Move` objects into a shared `InstrumentedQueue`, which is consumed by the dedicated `control_loop` thread. This modular architecture, separating perception, planning, and execution into distinct threads and data structures, is a key architectural decision that should provide a robust foundation for future complexity.

Several implementation details and challenges are evident in this commit. We've fine-tuned the frame timings for several `virtual_controller.py` actions, particularly for various smash attacks, aerials, and special moves (e.g., `upward_attack`, `low_smash_attack`, `special_attack_air`). These adjustments are critical for character responsiveness and attack effectiveness within the game's frame budget. The iterative refinements to the `prompt_text` fed to the LLM reveal ongoing experimentation with how to best instruct the model to choose actions, ultimately settling on a simplified "Select the next eight actions" directive. Crucially, while the LLM *receives* a full screenshot, the visual interpretation provided to it is currently a hardcoded string ("Mario is to the left of Donkey Kong"). This suggests a deliberate decoupling: we're first validating the LLM's ability to ingest multimodal data and output valid tool calls, deferring the full integration of the Roboflow-based object detection and game state summarization. Finally, the `dummy_planner` has been repurposed to iterate through almost *all* available attack and special moves (excluding basic movements for now), effectively acting as a test harness for the broader set of actions the LLM is now capable of invoking. The `llm_planner` is uncommented in `main`, indicating that our first LLM agent is now active, running with an aggressive `loop_delay_seconds = 0` for initial testing.

### Commit 16: Add random video (5eedadf)

Here's a detailed `NOTES.md` entry for commit `5eedadf`:

## NOTES.md: Commit 5eedadf - Add random video

This commit marks a crucial step in fleshing out the core "game loop" functionality outlined in our `TODO.md`, specifically the requirement to run for "hours" and handle an "agent" perspective with scorekeeping. The immediate problem being solved here wasn't about complex game logic, but rather a more fundamental one: how do we provide *any* visual feedback or engagement when the system is running for extended periods, especially since much of the core gameplay (combos, win conditions) is still under active development? We needed a low-effort way to confirm the display pipeline was functional and to avoid a blank screen during long test runs or attract modes.

Our technical approach was pragmatic: introduce a pre-recorded video as a placeholder. The `dk-random.mp4` file has been added to `smash/slides/public/`. This placement immediately leverages our existing static asset serving mechanism; anything in `public/` is directly accessible to the front-end or display layer. The specific path, `smash/slides/`, is also quite telling: it strongly suggests a conceptual architecture where our long-running display isn't just a single static screen but potentially a sequence of "slides" that can incorporate various media types, of which video is now a primary contender. This implicitly establishes video as a first-class citizen in our visual display pipeline, a key architectural decision for future media integration.

Ultimately, `dk-random.mp4` serves as both a proof-of-concept and a temporary filler. Its "random" nature (likely a generic Donkey Kong clip, given the `dk` prefix) means it's not tied to any specific game state or outcome, making it ideal for an initial attract mode or background loop. This allows us to start building and testing the display integration for video playback without needing to generate dynamic game-specific content just yet. It enables us to move forward on the "Game loop" `TODO` item by providing the visual element needed for continuous operation, paving the way for more sophisticated, context-aware video sequences—like actual gameplay highlights or victory/game-over screens—to be integrated later.

### Commit 17: feat(slides): Add slide for vision-enabled LLM (a9ff390)

Here's a `NOTES.md` entry for commit `a9ff390`:

---

### Commit: `a9ff390` - `feat(slides): Add slide for vision-enabled LLM`

This commit marks a significant leap in our agent's perception capabilities, specifically addressing the Achilles' heel of pure LLM-based spatial reasoning. Previously, our `llm_planner` would feed raw screenshots directly to Gemini, asking it to interpret the scene and suggest actions. As noted, Gemini's "spatial reasoning is... aspirational"—meaning, it often struggled to accurately determine relative positions of characters, leading to suboptimal or nonsensical moves. The problem we're solving here is providing our LLM with structured, accurate environmental context to make truly intelligent decisions.

The technical approach taken is a hybrid AI architecture, clearly separating the tasks of low-level perception from high-level strategic reasoning. Instead of burdening the LLM with pixel-by-pixel analysis, we've introduced a dedicated vision model (Roboflow) into the pipeline. This model's sole responsibility is to accurately detect and localize key entities, such as Mario and Donkey Kong, within the game screen. A crucial architectural decision was the implementation of a `summarize_smash_detection` function. This function acts as a vital translation layer, converting the raw bounding box coordinates and object detections from Roboflow into a concise, human-readable description (e.g., "Mario: x=100 | DK: x=300 | dx=200"). This symbolic summary is then prepended to the LLM's prompt, effectively giving Gemini the precise "state" of the game it needs to reason effectively.

From an implementation perspective, the `llm_planner`'s loop has been enhanced: it now `capture_screenshot()`, then passes this PNG to `call_roboflow_inference()`, and subsequently feeds the `result` into `summarize_smash_detection()`. The `prompt` for Gemini dynamically incorporates this `desc` (e.g., `prompt = f"State: {desc}. Select your next eight actions."`), ensuring its strategic choices are grounded in accurate perception. The `virtual_controller.py` diff clearly shows the uncommenting of the live Roboflow and summarization calls, replacing the previous hardcoded spatial description. This entire workflow is beautifully illustrated in the new "Step 4: Integrate vision with Gemini" slide, complete with the `/dk-vision-one.mp4` video demonstrating Donkey Kong's newfound spatial awareness and significantly more intentional, reactive behavior. This modular approach overcomes the inherent limitations of a single, monolithic LLM trying to do everything, proving that sometimes, two specialized AI brains are better than one generalist.

### Commit 18: feat(slides): Add slides for debugging and Q&A (3cfca62)

### 2025-07-01 - 3cfca62: Enhancing Presentation Storytelling: Debugging & Vision Walkthrough

This commit marks a significant iteration on our presentation slides, specifically honing in on how we articulate the development story of our LLM-powered agent. The core problem we aimed to solve here was two-fold: first, to clearly demonstrate the debugging process for such a complex, agentic system, moving beyond abstract concepts to concrete tools; and second, to more effectively showcase the nuanced impact of our agent's vision capabilities. Essentially, it was about making our internal development challenges and breakthroughs palpable to an audience.

From a technical storytelling perspective, the standout addition is the dedicated "Step 5: Debugging with LangSmith" slide. This isn't just about throwing a tool name on the screen; it's about explaining *how* we debug. We've included a concise bash snippet for setting up LangSmith tracing, coupled with a crucial new video (`dk-langsmith.mp4`) that walks through a real LangSmith trace. This directly addresses the inherent challenge of debugging LLM agents: it's rarely about simple code errors, but often about diagnosing *a belief*. As our internal notes suggest, we had instances where "Gemini thought Mario was still there" when he clearly wasn't. LangSmith provides the invaluable visibility into the exact prompts (including image context) and the resulting tool calls, allowing us to pinpoint these "belief discrepancies" and understand the LLM's internal reasoning process. This reinforces LangSmith not just as a monitoring tool, but as an integral part of our development and understanding of the agent's emergent behavior.

Beyond debugging, we've leveled up the demonstration of our agent's vision capabilities. Previously, we might have shown a single video of the agent reacting to its environment. Now, by leveraging the `<v-switch>` component, we can dynamically toggle between multiple video examples (`dk-vision-one.mp4`, `dk-vision-two.mp4`) demonstrating the vision-enabled agent's more intentional and reactive behavior. This is a subtle but powerful enhancement to the presentation, allowing us to illustrate different scenarios or the evolution of the agent's understanding, rather than just a single static example. Additionally, we've streamlined the "Live Demo" section, likely to ensure a smoother transition into the live performance and allow the demo itself to be the primary focus rather than extensive textual setup.

Finally, the commit wraps up with a polished "Thanks!" slide featuring a direct QR code link (`github-qr.svg`) to the project repository. This simple but effective implementation detail ensures that curious audience members can immediately dive deeper into the code and contribute, closing the loop on our presentation with a clear call to action and a nod to our open-source commitment. While no new agent code was shipped here, this commit dramatically refines how we communicate the sophisticated internal workings and the practical development journey of our agent.

### Commit 19: Add the QR. (462f4b0)

## NOTES.md Entry: `462f4b0: Add the QR.`

This commit, though seemingly minor at first glance, addresses a crucial aspect of project visibility and dissemination rather than direct game mechanics. While the primary focus of development remains on core gameplay elements like the "Game loop" and "Combos" as outlined in `TODO.md`, a complete project lifecycle also involves showcasing progress and enabling community engagement.

The problem being solved here is straightforward: how to easily direct an audience, likely during a presentation or demonstration, to the project's GitHub repository for code access, contributions, or further information. The technical approach was equally direct: generating a QR code (a `github-qr.png` image) that, when scanned, would lead directly to the repository. This image was then integrated into the project's asset structure.

A key architectural decision was the placement of this asset within `smash/slides/public/`. This indicates a dedicated structure for presentation materials, with a `public` sub-directory for assets that are meant to be served or displayed alongside the slides themselves. This separation of concerns is a good practice, ensuring that presentation-specific assets don't clutter core game logic or other application resources. While there were no complex coding challenges here, the "overcome" aspect lies in proactive preparation for project outreach and ensuring the project is easily discoverable in a live setting.

In essence, this commit represents a pivot from internal development concerns to external communication. It ensures that when the time comes to present the "Game loop" in action, a seamless pathway exists for interested parties to dive deeper into the project's codebase. It's a small but significant step towards making the `smash` project accessible and shareable.

### Commit 20: Add license (d0ddacf)

# NOTES.md

---

**Commit: `d0ddacf` - Add license**

This commit marks a crucial foundational step for the `smash` project, shifting focus from purely functional development to establishing proper project governance. While the `TODO.md` rightly highlights the core technical challenges ahead—like perfecting "Combos" and building out the robust "Game loop" to enable agent-driven, long-running matches—this commit addresses the equally critical, albeit less glamorous, aspect of **intellectual property and project distribution.**

The problem being solved here is the **lack of a clear legal framework** for the codebase. Without a license, the project's ownership and usage terms are ambiguous, potentially hindering collaboration, adoption, and future open-source release. By adding a license, we're explicitly defining how others (and even internal teams) can use, modify, and distribute the software, and importantly, asserting the copyright for `Google LLC`. This isn't just a compliance formality; it's a strategic decision that enables the project to grow and potentially be shared, laying the groundwork for how contributions will be managed and how the fruit of our labor (like that long-anticipated "game over screen (agent)" logic!) can be legally consumed.

Our technical approach for this was straightforward but thorough: **applying the Apache License 2.0 boilerplate** to virtually every source file within the `smash/` directory. This involved carefully adapting the standard license text to the specific comment syntax of each file type—from Python (`.py`) and shell scripts (`.sh`) to Dockerfiles, and even frontend assets like Vue components (`.vue`), TypeScript snippets (`.ts`), and CSS stylesheets (`.css`). The choice of Apache 2.0 as the **key architectural decision** here provides a balance of openness and protection; it's a permissive license allowing broad use and modification, while also including patent grants and clear attribution requirements, which is often a preferred choice for corporate-backed open-source projects. No significant technical challenges were encountered beyond ensuring consistent application and correct comment formatting across the diverse set of file types.

This commit, while not directly touching the game logic or AI, is an indispensable prerequisite for the project's long-term health and potential public-facing future. It defines the rules of engagement, ensuring that as we dive deeper into features like advanced "combos" and a resilient "game loop," the legal scaffolding is firmly in place, giving us the freedom to build and iterate without IP concerns looming.
